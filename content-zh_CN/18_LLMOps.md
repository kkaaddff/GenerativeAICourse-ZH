# LLMOps: 端到端的AI应用生命周期管理

## 什么是LLMOps？

我们之前已经有了像**DevOps**这样的实践，它是一个用于开发、测试和部署应用程序的端到端框架，包含了一套工具（如GitHub、Jenkins等）。**LLMOps**是一个类似的概念，它是**一套用于开发、部署和维护基于LLM的应用的工具和流程集合**。

根本的区别在于，传统的DevOps处理的是**确定性代码**（相同的输入总是产生相同的输出），而LLMOps处理的是**概率性AI系统**，其中输出可能会变化，模型需要重新训练，并且数据质量直接影响性能。

## LLMOps流水线：7个关键阶段

![image](https://github.com/user-attachments/assets/a5b44cbf-7375-4199-8665-a2dc54d179dd)

### 1. 数据策管 (Data Curation): AI成功的基石

这个阶段探索哪些数据是可用的，并规划数据转换以创建干净、一致的数据。你需要丰富数据吗？你需要将数据与外部数据映射以丰富它吗？这些都是数据策管过程的一部分。

**挑战：** 原始的企业数据是混乱、不一致且常常不完整的。在任何AI模型能够有效工作之前，你需要将这些数据转换为能够产生可靠结果的格式。

#### 用于数据策管的Azure工具：

-   **Azure Data Factory (ADF):**
    -   **用途：** 协调来自多个来源的数据管道。
    -   **功能：** 90多个内置连接器，可视化管道设计器，自动化调度。
    -   **用例：** 从SAP、Salesforce、Oracle、Excel文件中提取数据，并为AI消费进行转换。

-   **Microsoft Fabric:**
    -   **用途：** 统一的分析平台，结合了数据工程、数据仓库和实时分析。
    -   **功能：** OneLake数据湖、Spark笔记本、实时流处理、T-SQL端点。
    -   **用例：** 处理数百万条客户记录，清洗交易数据，大规模执行复杂连接。

-   **Microsoft Purview:**
    -   **用途：** 在整个数据资产中进行数据治理、发现和血缘追踪。
    -   **功能：** 自动数据分类、PII检测、数据血缘可视化、数据目录。
    -   **用例：** 对所有企业数据源进行编目，为合规性追踪数据血缘，识别敏感数据。

### 2. 实验 (Experimentation): 规模化的试错

这是你用不同的数据、不同的提示、不同的模型等来运行你的LLM解决方案的时候。我从客户那里收到的最常见的问题之一是：我怎么知道我的数据是否适合AI？答案是**试错**。有一些基本准则，我们在上一节中讨论过，但对于AI来说，试错是极其重要的。

#### 用于实验的Azure工具：

-   **Azure AI Studio:**
    -   **用途：** 统一的生成式AI开发平台。
    -   **功能：** Prompt flow设计器、模型中心访问、评估指标、RAG管道测试。
    -   **用例：** 比较GPT-4o、Claude、Llama的响应，测试提示变体，优化RAG。

-   **Azure OpenAI Service:**
    -   **用途：** 访问具有企业控制和合规性的OpenAI模型。
    -   **功能：** GPT-4o, GPT-4 Turbo, GPT-3.5 Turbo, DALL-E 3, Whisper, text-embedding-3-large。
    -   **用例：** 在公司数据上微调模型，用公司知识库实现RAG。

-   **Azure AI模型目录 (通过AI Studio):**
    -   **用途：** 访问来自多个提供商的多样化基础模型。
    -   **功能：** Llama 3.1, Mistral Large, Phi-3, Cohere Command等50多种模型。
    -   **用例：** OpenAI模型的成本效益替代方案，特定领域的专用模型。

### 3. 评估 (Evaluation): 衡量重要之事

这是定义指标并观察变化如何影响它们的过程。与传统软件测试bug不同，AI评估需要衡量主观质量、准确性和业务影响。

#### 用于评估的Azure工具：

-   **Azure AI Studio评估:**
    -   **用途：** 用于生成式AI应用的综合评估套件。
    -   **功能：** 内置指标（如真实性、相关性、连贯性），自定义评估器，AI安全评估。
    -   **用例：** 自动为RAG响应的准确性打分，测试有害内容，衡量响应质量。

-   **Azure Monitor + Application Insights:**
    -   **用途：** 跟踪AI应用的应用性能和用户行为。
    -   **功能：** 自定义指标、实时仪表板、异常检测、分布式追踪。
    -   **用例：** 监控响应时间、令牌使用量、错误率、用户参与模式。

-   **Azure AI内容安全:**
    -   **用途：** 检测和过滤AI应用中的有害内容。
    -   **功能：** 实时安全分类、自定义内容策略、严重性评分。
    -   **用例：** 在向用户展示前评估AI输出的仇恨言论、暴力、色情内容，确保品牌安全。

### 4. 验证与部署 (Validate & Deploy): 生产就绪

模型在生产中的表现如何？这时会进行一系列A/B测试，以确保AI系统在真实用户和真实数据下能够可靠地工作。

#### 用于验证与部署的Azure工具：

-   **Azure容器实例 (ACI) / Azure Kubernetes服务 (AKS):**
    -   **用途：** 在可扩展的容器中部署AI应用。
    -   **功能：** 自动扩展、负载均衡、蓝绿部署。
    -   **用例：** 部署RAG应用，托管自定义AI模型。

-   **Azure API管理:**
    -   **用途：** 管理、保护和监控AI API端点。
    -   **功能：** 速率限制、认证、使用分析、A/B测试。
    -   **用例：** 为AI服务创建安全端点，实施逐步推广。

-   **Azure DevOps:**
    -   **用途：** AI应用的CI/CD管道。
    -   **功能：** 自动化测试、部署管道、发布管理。
    -   **用例：** 自动化模型部署，在生产发布前运行评估测试。

### 5. 推理 (Inference): 生产中可靠的AI

确保AI响应是可靠、一致且低延迟的。这涉及到管理向最终用户实时提供AI模型服务的过程。

#### 用于推理的Azure工具：

-   **Azure OpenAI Service:**
    -   **用途：** 具有企业级SLA和数据驻留的生产就绪OpenAI模型。
    -   **功能：** 99.9%的正常运行时间SLA，专用容量（预配吞吐量单位），区域部署。
    -   **用例：** 需要保证容量和性能的大容量应用。

-   **Azure AI模型即服务 (MaaS):**
    -   **用途：** 使用托管基础设施部署第三方基础模型。
    -   **功能：** Llama 3.1 405B, Mistral Large 2, Cohere Command R+，具有Azure安全和计费功能。
    -   **用例：** 针对特定用例的OpenAI模型的成本效益替代方案，满足合规性要求。

-   **Azure AI内容安全:**
    -   **用途：** 对生成式AI进行实时内容过滤和安全检查。
    -   **功能：** 检测仇恨言论、暴力、色情内容、自残，自定义黑名单。
    -   **用例：** 在向用户展示前过滤AI输出，遵守内容政策和法规。

### 6. 监控 (Monitor): 实时智能

包括实时警报、查询、利用率、成本和性能指标，以确保你的AI系统持续有效工作。

#### 用于监控的Azure工具：

-   **Azure Monitor:**
    -   **用途：** AI应用的综合监控平台。
    -   **功能：** 指标、日志、警报、仪表板。
    -   **用例：** 跟踪API响应时间、令牌使用量、错误率。

-   **Azure Log Analytics:**
    -   **用途：** 查询和分析应用日志。
    -   **功能：** KQL查询、自定义仪表板、异常检测。
    -   **用例：** 调查用户问题，分析使用模式，排除故障。

-   **Azure成本管理:**
    -   **用途：** 跟踪和优化AI支出。
    -   **功能：** 预算警报、成本分析、使用建议。
    -   **用例：** 监控令牌成本，设置支出限制，优化模型使用。

### 7. 反馈 (Feedback): 持续改进

我们如何捕获用户反馈，同时确保隐私和合规？用户反馈对于改进AI系统至关重要，但必须负责任地收集。

#### 用于反馈的Azure工具：

-   **Azure AI语言:**
    -   **用途：** 分析用户反馈文本以获取见解和情感（Text Analytics的后继者）。
    -   **功能：** 情感分析、观点挖掘、关键短语提取、实体识别。
    -   **用例：** 自动分类用户反馈，识别常见投诉，跟踪满意度趋势。

-   **Azure AI文档智能:**
    -   **用途：** 从用户反馈表和文档中提取数据（Form Recognizer的后继者）。
    -   **功能：** OCR、布局分析、自定义模型训练、常见表单的预建模型。
    -   **用例：** 处理手写反馈表，从调查中提取结构化数据。

-   **Azure事件中心:**
    -   **用途：** 实时反馈数据摄取和流分析。
    -   **功能：** 流处理、实时分析、与Fabric和AI服务的集成。
    -   **用例：** 收集实时用户互动、反馈事件、行为数据。

## LLMOps的关键成功因素：

1.  **从小处着手：** 在扩展之前，从一个用例和经过验证的工具开始。
2.  **衡量一切：** 从第一天起就进行全面的监控。
3.  **业务对齐：** 将技术指标与业务成果联系起来。
4.  **迭代改进：** 定期进行实验和优化。
5.  **跨职能团队：** 包括领域专家，而不仅仅是技术人员。
6.  **合规优先：** 从一开始就内置隐私和安全。

LLMOps不仅仅是关于工具——它是关于创建一个系统化的方法来构建、部署和维护能够提供持续业务价值的AI系统，同时管理概率性AI系统带来的独特挑战。
