{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb2a0fd0",
      "metadata": {},
      "source": [
        "\n",
        "# 使用Python调用OpenAI API构建聊天机器人\n",
        "\n",
        "在之前的实验中，我们学习了如何在本地运行LLM。现在，我们将学习如何通过官方API与OpenAI强大的语言模型进行交互，特别是使用其为对话设计的 `chat completions` 端点。\n",
        "\n",
        "## 1. 准备工作\n",
        "\n",
        "在运行本笔记之前，你需要先设置好对OpenAI API的访问权限。\n",
        "\n",
        "### 1.1 获取OpenAI API密钥\n",
        "\n",
        "要使用OpenAI API，你必须拥有一个API密钥：\n",
        "\n",
        "1. 在 [OpenAI官网](https://openai.com/) 创建一个账户。\n",
        "2. 导航至 [API密钥页面](https://platform.openai.com/api-keys)。\n",
        "3. 创建一个新的密钥。\n",
        "4. **请务必妥善保管好这个密钥**——它就像你的密码一样重要！\n",
        "\n",
        "### 1.2 配置你的环境\n",
        "\n",
        "首先，安装OpenAI的Python官方库："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be6a1ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0763dd",
      "metadata": {},
      "source": [
        "然后，设置你的API密钥。为了安全起见，最佳实践是使用环境变量来管理密钥。我们推荐创建一个名为 `.env` 的文件来存放它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63831fc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# 从.env文件加载环境变量\n",
        "# 请确保你的.env文件中有这样一行：OPENAI_API_KEY=sk-xxxxxxxxxx \n",
        "load_dotenv()\n",
        "\n",
        "# 初始化OpenAI客户端。它会自动从环境变量中读取API密钥。\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc743e92",
      "metadata": {},
      "source": [
        "## 2. 使用Chat Completions API\n",
        "\n",
        "OpenAI提供的 `Chat Completions API` 专为对话式交互而设计。调用它时，我们需要提供一个消息列表，其中通常包含**系统消息**（设定模型的行为方式）和**用户消息**（用户的输入）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11788be4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 我们使用 client.chat.completions.create 方法，它接收一个消息列表（messages）和模型名称（model）。\n",
        "# 消息列表是一个由字典组成的数组，每个字典都包含 'role' 和 'content' 两个键。\n",
        "# 第一个消息通常是 'system' 角色的系统消息，用于设定助手的行为。\n",
        "# 后续消息是 'user' 角色的用户输入。\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # 你也可以选择 gpt-3.5-turbo 等其他模型\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"你是一个乐于助人的助手。\"},\n",
        "        {\"role\": \"user\", \"content\": \"写一个关于独角兽的单句睡前故事。\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 响应对象 `response` 包含一个 `choices` 列表，我们通常取第一个选择（choices[0]）。\n",
        "# 然后从该选择的 `message` 属性中获取我们想要的 `content`。\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3dfed48",
      "metadata": {},
      "source": [
        "## 3. 理解响应结构\n",
        "\n",
        "API返回的是一个结构化的响应对象，其中包含了许多有用的元数据。让我们打印出来看看："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dafe541a",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"你是一个乐于助人的助手。\"},\n",
        "        {\"role\": \"user\", \"content\": \"你好，你今天过得怎么样？\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 打印完整的响应对象结构\n",
        "print(\"完整的响应对象:\")\n",
        "print(response)\n",
        "\n",
        "# 访问特定的部分\n",
        "print(\"\n仅消息内容:\")\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "print(\"\n使用的模型:\")\n",
        "print(response.model)\n",
        "\n",
        "print(\"\n使用量统计:\")\n",
        "print(f\"输入令牌数 (Prompt tokens): {response.usage.prompt_tokens}\")\n",
        "print(f\"输出令牌数 (Completion tokens): {response.usage.completion_tokens}\")\n",
        "print(f\"总令牌数 (Total tokens): {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cbe241",
      "metadata": {},
      "source": [
        "从响应中，我们可以看到许多有用的数据。最重要的是，我们能清楚地了解这次API调用消耗了多少**令牌（tokens）**。模型的定价通常基于输入和输出令牌的总和，所以在本例中，我们为54个令牌付费。我们将在本实验的最后更详细地讨论定价问题。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a99f65d0",
      "metadata": {},
      "source": [
        "## 4. 创建一个简单的聊天界面（无记忆）\n",
        "\n",
        "让我们先构建一个没有记忆功能的聊天机器人。\n",
        "\n",
        "在Jupyter Notebook环境中，我们无法创建一个持续等待输入的聊天循环（即 `input()` 循环）。因此，我们将创建一个简单的函数，并在不同的单元格中调用它，以模拟一次性的、无记忆的聊天交互。\n",
        "\n",
        "### 单元格 1: 定义函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e6dbed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 这个函数接收一个用户消息作为输入，并从OpenAI API返回一个响应。\n",
        "def get_response_no_memory(user_message):\n",
        "    \"\"\"从OpenAI获取响应（不包含对话历史）\"\"\"\n",
        "    # 这是一个文档字符串（docstring），用于描述函数的功能和参数，对于代码的可读性和文档生成非常有用。\n",
        "\n",
        "    # 我们用用户消息调用模型\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"你是一个乐于助人的助手。\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90c6ea6",
      "metadata": {},
      "source": [
        "### 单元格 2: 第一次交互"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3391e7fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第一个问题（你可以输入任何内容）\n",
        "question = \"什么是人工智能？\"\n",
        "answer = get_response_no_memory(question)\n",
        "\n",
        "print(f\"你: {question}\")\n",
        "print(f\"助手: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d9856a",
      "metadata": {},
      "source": [
        "### 单元格 3: 后续问题（无记忆）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba3c502",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第二个问题（AI不会记得上一次的交互）\n",
        "question = \"你能就此详细阐述一下吗？\"\n",
        "answer = get_response_no_memory(question)\n",
        "\n",
        "print(f\"你: {question}\")\n",
        "print(f\"助手: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129fad52",
      "metadata": {},
      "source": [
        "你会注意到，当你问“你能就此详细阐述一下吗？”，助手不知道“此”指的是什么，因为它对上一次的交流**毫无记忆**。\n",
        "\n",
        "## 5. 创建一个有记忆的聊天界面\n",
        "\n",
        "现在，让我们构建一个能够维护对话历史的版本。\n",
        "\n",
        "### 单元格 1: 设置对话记忆"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e03bf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 用系统消息初始化对话列表。我们将随着对话的进行，向这个列表中添加更多的消息。\n",
        "conversation_memory = [\n",
        "    {\"role\": \"system\", \"content\": \"你是一个乐于助人的助手。\"}\n",
        "]\n",
        "\n",
        "# 定义一个函数，用于将用户消息添加到历史记录中，并从OpenAI获取响应\n",
        "def chat_with_memory(user_message):\n",
        "    \"\"\"在维护对话历史的同时与AI聊天\"\"\"\n",
        "    \n",
        "    # 1. 将用户的消息添加到历史记录中\n",
        "    conversation_memory.append({\"role\": \"user\", \"content\": user_message})\n",
        "    \n",
        "    # 2. 使用完整的对话历史调用OpenAI API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=conversation_memory\n",
        "    )\n",
        "    \n",
        "    # 3. 提取助手的响应内容\n",
        "    assistant_response = response.choices[0].message.content\n",
        "    \n",
        "    # 4. 将助手的响应也添加到历史记录中，为下一次交互做准备\n",
        "    conversation_memory.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "    \n",
        "    # 5. 返回响应内容和本次交互消耗的令牌数\n",
        "    return assistant_response, response.usage.total_tokens\n",
        "\n",
        "# 定义一个函数来显示对话历史。我们遍历历史列表并打印每条消息，跳过系统消息以保持输出整洁。\n",
        "def show_conversation():\n",
        "    \"\"\"显示当前的对话内容\"\"\"\n",
        "    for message in conversation_memory:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            continue  # 跳过系统消息\n",
        "        # 将角色首字母大写以美化输出，例如 'user' -> 'User'\n",
        "        print(f\"{message['role'].capitalize()}: {message['content']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25889352",
      "metadata": {},
      "source": [
        "### 单元格 2: 第一次有记忆的交互"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d6ae066",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第一个问题\n",
        "question = \"什么是人工智能？\"\n",
        "answer, tokens = chat_with_memory(question)\n",
        "\n",
        "print(f\"你: {question}\")\n",
        "print(f\"助手: {answer}\")\n",
        "print(f\"[消耗令牌数: {tokens}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc48f122",
      "metadata": {},
      "source": [
        "### 单元格 3: 后续问题（有记忆）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010d52d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第二个问题（现在AI会记得上一次的交互了）\n",
        "question = \"你能就此详细阐述一下吗？\"\n",
        "answer, tokens = chat_with_memory(question)\n",
        "\n",
        "print(f\"你: {question}\")\n",
        "print(f\"助手: {answer}\")\n",
        "print(f\"[消耗令牌数: {tokens}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3ef1ac",
      "metadata": {},
      "source": [
        "### 单元格 4: 查看完整对话"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "236daba6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 查看至今为止的完整对话历史\n",
        "print(\"完整对话历史:\")\n",
        "print(\"-\" * 30)\n",
        "show_conversation()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b7cc3d",
      "metadata": {},
      "source": [
        "### 单元格 5: 重置对话（如果需要）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcfbe877",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 如果你想重新开始一段对话，可以重置对话历史\n",
        "conversation_memory = [\n",
        "    {\"role\": \"system\", \"content\": \"你是一个乐于助人的助手。\"}\n",
        "]\n",
        "print(\"对话已重置！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18571575",
      "metadata": {},
      "source": [
        "## 6. 流式响应 (Streaming Responses)\n",
        "\n",
        "流式响应可以让你在模型生成答案的同时，实时地看到输出，就像打字机一样，而不是等待它一次性生成全部内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57923783",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def stream_response(user_message):\n",
        "    \"\"\"从OpenAI流式获取响应，但不存储对话历史\"\"\"\n",
        "    \n",
        "    # 通过设置 `stream=True`，API会以数据块（chunks）的形式返回响应。\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"你是一个乐于助人的助手。\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    print(f\"你: {user_message}\")\n",
        "    print(\"助手: \", end=\"\", flush=True)  # `end=\"\"` 表示不换行，`flush=True` 确保立即打印输出。\n",
        "    \n",
        "    # 处理数据流\n",
        "    full_response = \"\" # 初始化一个空字符串，用于拼接完整的响应。\n",
        "    for chunk in stream: # 遍历数据流中的每一个数据块。\n",
        "        # 检查数据块中是否有内容。\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            content_chunk = chunk.choices[0].delta.content # 获取这部分响应内容。\n",
        "            full_response += content_chunk # 将其拼接到完整响应中。\n",
        "            print(content_chunk, end=\"\", flush=True) # 实时打印这部分内容。\n",
        "            time.sleep(0.01)  # 添加微小延迟，使输出更易读。\n",
        "    \n",
        "    print(\"\\n\")  # 响应结束后换行。\n",
        "    return full_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67023a00",
      "metadata": {},
      "source": [
        "测试流式函数："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c45753a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 尝试流式函数\n",
        "stream_response(\"写一首关于编程的短诗。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd38435",
      "metadata": {},
      "source": [
        "## 7. 带记忆的流式响应\n",
        "\n",
        "让我们将流式响应与对话记忆结合起来："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5fb54bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 初始化带记忆的流式对话历史\n",
        "streaming_conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"你是一个乐于助人的助手。\"}\n",
        "]\n",
        "\n",
        "def stream_chat_with_memory(user_message):\n",
        "    \"\"\"带记忆的流式聊天\"\"\"\n",
        "    \n",
        "    # 将用户消息添加到历史记录\n",
        "    streaming_conversation.append({\"role\": \"user\", \"content\": user_message})\n",
        "    \n",
        "    # 获取流式响应\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=streaming_conversation,\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    print(f\"你: {user_message}\")\n",
        "    print(\"助手: \", end=\"\", flush=True)\n",
        "    \n",
        "    # 处理数据流\n",
        "    assistant_response = \"\"\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            content_chunk = chunk.choices[0].delta.content\n",
        "            assistant_response += content_chunk\n",
        "            print(content_chunk, end=\"\", flush=True)\n",
        "            time.sleep(0.01)\n",
        "    \n",
        "    print(\"\\n\")  # 响应结束后换行\n",
        "    \n",
        "    # 将助手的完整响应添加到历史记录\n",
        "    streaming_conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "    \n",
        "    return assistant_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3787019",
      "metadata": {},
      "source": [
        "测试带记忆的流式聊天："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243d1a54",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第一个带记忆的流式问题\n",
        "stream_chat_with_memory(\"机器人三定律是什么？\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070f8a66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 后续的带记忆的流式问题\n",
        "stream_chat_with_memory(\"是谁创造了这些定律？\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad8e62f",
      "metadata": {},
      "source": [
        "## 8. 理解不同的消息角色\n",
        "\n",
        "OpenAI Chat API在消息中主要使用三种角色："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80062ffb",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        # 系统 (system) 消息 - 设定行为和上下文\n",
        "        {\"role\": \"system\", \"content\": \"你是一个只会用海盗黑话说话的海盗。\"},\n",
        "        \n",
        "        # 用户 (user) 消息 - 用户说的话\n",
        "        {\"role\": \"user\", \"content\": \"你好，今天过得怎么样？\"},\n",
        "        \n",
        "        # 助手 (assistant) 消息 - 助手之前的回复，用于提供对话历史\n",
        "        {\"role\": \"assistant\", \"content\": \"啊哈！我今天感觉棒极了，我的好伙计！\"},\n",
        "        \n",
        "        # 另一个用户消息\n",
        "        {\"role\": \"user\", \"content\": \"跟我说说天气怎么样。\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4248540b",
      "metadata": {},
      "source": [
        "## 9. 理解上下文窗口\n",
        "\n",
        "不同的OpenAI模型有不同的上下文窗口限制：\n",
        "\n",
        "- **GPT-3.5-Turbo**: 4,096 或 16,384 令牌（取决于具体版本）\n",
        "- **GPT-4**: 8,192 或 32,768 令牌（取决于具体版本）\n",
        "- **GPT-4-Turbo / GPT-4o**: 高达 128,000 令牌\n",
        "\n",
        "与本地模型不同，OpenAI会为你管理令牌：\n",
        "1. 如果你发送的对话历史超出了模型的上下文窗口限制，API将返回错误。\n",
        "2. 你的费用是根据你使用的令牌数量计算的。\n",
        "3. API在每次请求的响应中都会提供令牌使用统计。\n",
        "\n",
        "让我们看看令牌在实际中的作用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38fb22a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建一个更长的对话\n",
        "long_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是一个乐于助人的助手。\"}\n",
        "]\n",
        "\n",
        "# 向历史记录中添加一些消息\n",
        "for i in range(5):\n",
        "    long_messages.append({\"role\": \"user\", \"content\": f\"这是测试消息 {i+1}。告诉我一些关于太空的有趣事实。\"})\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=long_messages\n",
        "    )\n",
        "    assistant_msg = response.choices[0].message.content\n",
        "    long_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "    print(f\"第 {i+1} 轮交互 - 总令牌数: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1793337a",
      "metadata": {},
      "source": [
        "## 10. 管理成本和令牌\n",
        "\n",
        "使用OpenAI API时，你需要时刻关注成本：\n",
        "\n",
        "1. **令牌计量**: 每次请求和响应都会消耗令牌，而你需要为这些令牌付费。\n",
        "2. **模型选择**: 更强大的模型（如GPT-4o）通常比基础模型（如GPT-3.5-Turbo）每令牌的成本更高。\n",
        "3. **上下文窗口**: 更长的对话会发送更多的令牌，因此成本也更高。\n",
        "\n",
        "管理成本的一些技巧："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bf00caa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 对复杂度较低的任务使用更便宜的模型\n",
        "response_cheap = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # 比 GPT-4o 便宜\n",
        "    messages=[{\"role\": \"user\", \"content\": \"总结一下锻炼的好处。\"}]\n",
        ")\n",
        "print(f\"GPT-3.5-Turbo 成本: {response_cheap.usage.total_tokens} 令牌\")\n",
        "\n",
        "# 2. 控制最大令牌数以限制响应长度和成本\n",
        "response_limited = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"给我讲讲量子物理学。\"}],\n",
        "    max_tokens=100  # 限制响应长度最多为100个令牌\n",
        ")\n",
        "print(f\"受限响应成本: {response_limited.usage.total_tokens} 令牌\")\n",
        "\n",
        "# 3. 使用温度（temperature）控制随机性。较高的值使输出更具创造性，较低的值使其更具确定性。\n",
        "response_creative = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"写一个有创意的故事。\"}],\n",
        "    temperature=0.8  # 较高的温度以获得更有创意的输出\n",
        ")\n",
        "print(f\"创意响应成本: {response_creative.usage.total_tokens} 令牌\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc080ade",
      "metadata": {},
      "source": [
        "## 11. 处理长对话的对话历史\n",
        "\n",
        "对于非常长的对话，你需要采取策略来管理上下文窗口，以避免超出限制并控制成本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e3ce521",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 策略1: 滑动窗口 - 只保留最近的N条消息\n",
        "def trim_conversation(messages, max_messages=10):\n",
        "    # 总是保留第一条系统消息\n",
        "    if len(messages) > max_messages + 1:\n",
        "        system_message = messages[0]\n",
        "        recent_messages = messages[-(max_messages):] # 取最后N条消息\n",
        "        return [system_message] + recent_messages\n",
        "    return messages\n",
        "\n",
        "# 策略2: 对话摘要 - 定期用AI总结对话，并用摘要替换旧消息\n",
        "def summarize_conversation(messages):\n",
        "    # 将对话历史拼接成一个字符串\n",
        "    conversation_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages if m['role'] != 'system'])\n",
        "    \n",
        "    # 创建一个摘要请求\n",
        "    summary_request = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"请简明扼要地总结以下对话。\"},\n",
        "            {\"role\": \"user\", \"content\": conversation_text}\n",
        "        ]\n",
        "    )\n",
        "    summary = summary_request.choices[0].message.content\n",
        "    \n",
        "    # 用摘要替换原始对话\n",
        "    return [\n",
        "        messages[0],  # 保留原始的系统消息\n",
        "        {\"role\": \"system\", \"content\": f\"先前对话的摘要: {summary}\"}\n",
        "    ]\n",
        "\n",
        "# 何时使用：\n",
        "# if len(conversation_memory) > 20: # 例如，当对话超过20条消息时\n",
        "#     conversation_memory = summarize_conversation(conversation_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11edba9d",
      "metadata": {},
      "source": [
        "## 12. 本地LLM vs. OpenAI API 对比\n",
        "\n",
        "| 特性 | 本地LLM (Ollama) | OpenAI API |\n",
        "|---|---|---|\n",
        "| **设置** | 下载模型到本地，配置复杂 | 仅需API密钥，简单快捷 |\n",
        "| **成本** | 硬件成本，无按次付费 | 按令牌使用量付费 |\n",
        "| **隐私** | 数据保留在本地设备上 | 数据发送到OpenAI服务器 |\n",
        "| **性能** | 受限于本地硬件 | 可使用最先进的模型 |\n",
        "| **可靠性** | 取决于你的系统稳定性 | 由OpenAI管理的高可用服务 |\n",
        "| **上下文窗口** | 通常较小 | 高达128K甚至更多令牌 |\n",
        "| **内存管理** | 需要手动实现 | API层面部分处理，但仍需策略 |\n",
        "\n",
        "## 13. 进一步学习的资源\n",
        "\n",
        "- [OpenAI API 官方文档](https://platform.openai.com/docs/api-reference)\n",
        "- [OpenAI Cookbook (代码示例库)](https://github.com/openai/openai-cookbook)\n",
        "- [OpenAI Python 库](https://github.com/openai/openai-python)\n",
        "- [OpenAI 令牌计算器](https://platform.openai.com/tokenizer)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
