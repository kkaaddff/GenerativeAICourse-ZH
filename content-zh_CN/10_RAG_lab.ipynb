{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ad079d",
   "metadata": {},
   "source": [
    "# 使用Semantic Kernel实现完整的RAG系统\n",
    "\n",
    "本综合性笔记将演示如何使用微软的 **Semantic Kernel** 框架来构建一个检索增强生成（RAG）系统。我们将从展示没有特定数据访问权限的AI模型的局限性开始，然后逐步构建一个包含分块策略、向量数据库和评估方法的完整RAG系统。\n",
    "\n",
    "## 安装与设置\n",
    "\n",
    "在本模块中，我们将重点利用 **Semantic Kernel**。\n",
    "\n",
    "**Semantic Kernel** 是一个开源的SDK，旨在让开发者能轻松构建企业级的AI应用。它支持C#、Python和Java，已经为生产环境做好了准备，并被许多大型企业所采用。它的核心设计理念是**模块化**，这意味着你可以轻松地更换底层AI模型而无需重写整个代码库。\n",
    "\n",
    "像Semantic Kernel这样的SDK之所以流行，是因为LLM本身只能处理文本和生成响应，它无法直接访问你的数据库、调用你的API、执行代码或与外部系统交互。Semantic Kernel正是为了解决这个问题而生，它负责：\n",
    "- **管理与AI服务的连接**（如OpenAI、Azure OpenAI）。\n",
    "- 提供一个**插件（Plugin）系统**，让你可以编写供AI调用的自定义函数。\n",
    "- **管理对话历史和上下文**。\n",
    "\n",
    "Semantic Kernel的核心是**内核（Kernel）协调器**。在复杂的AI应用中，你需要协调多个活动部件，如AI服务、数据库、API、日志系统等。Kernel就像一个中央大脑，将所有这些部分整合在一起。它包含**服务（Services）**（如AI服务、日志服务）和**插件（Plugins）**（AI可以调用的自定义函数，如访问数据库）。想象一个真实的企业场景：一个AI助手需要查询CRM、检查库存、生成报价单，并为合规性记录所有交互。没有Kernel，每一段代码都需要知道如何连接所有这些服务。有了Kernel，所有配置只需一次。因为所有的AI操作都流经Kernel，你就拥有了一个用于日志记录和管理的单一控制点。\n",
    "\n",
    "**Semantic Kernel的关键组件:**\n",
    "\n",
    "1. **AI服务连接器**: 在一个多模型的世界里，不同的AI模型有不同的API和认证方法。SK中的连接器是一个抽象层，可以防止供应商锁定，让你在不同模型间轻松切换。\n",
    "2. **向量存储连接器**: 这是RAG的核心，是连接向量存储和Kernel的桥梁。\n",
    "3. **函数和插件**: 插件系统让LLM能够使用“工具”。一个**函数**是你暴露给LLM的单个能力（例如一个Python函数），而一个**插件**是一组相关函数的集合（例如，`DatabasePlugin`可能包含`GetUser`、`UpdateUser`等函数）。\n",
    "4. **提示模板**: 在代码中用多行字符串编写复杂的提示会变得混乱且难以维护。提示模板解决了这个问题，它允许你混合静态指令和动态占位符。\n",
    "5. **过滤器 (Filters)**: 一段代码，用于在Kernel执行的关键时刻进行拦截，例如在函数调用前或提示渲染后。你可以用它来过滤PII（个人身份信息），确保用户的敏感数据永远不会被发送到外部LLM。\n",
    "\n",
    "**首先，安装所需的软件包：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ed35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行此单元格以安装所有必需的包\n",
    "!pip install semantic-kernel openai numpy scikit-learn faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c7196",
   "metadata": {},
   "source": [
    "## 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 导入Semantic Kernel核心库\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "\n",
    "# 导入用于向量存储的库 - 我们将构建自己的简单系统\n",
    "import faiss # Facebook AI Similarity Search\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Semantic Kernel 环境设置完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43f176",
   "metadata": {},
   "source": [
    "## 创建我们的文档模型和向量存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15f662",
   "metadata": {},
   "source": [
    "我们将使用 **FAISS** 作为我们的向量数据库。FAISS 是一个完全在本地机器上运行的向量数据库库。在生产场景中，我们通常会使用云端的向量数据库，如 Azure AI Search。但其核心概念是相同的：向量数据库允许我们高效地存储和检索向量。\n",
    "\n",
    "**代码解释:** \n",
    "\n",
    "1.  我们将创建一个**类（Class）** `DocumentChunk` 来表示文档的每一个分块。类中的注释将详细解释每个字段的含义。\n",
    "2.  然后，我们将创建另一个类 `SimpleVectorStore`，它包含使用向量搜索（而非精确的关键词匹配）来查找这些文本块的**方法（函数）**。\n",
    "    -   **初始化函数 (`__init__`)**: 用于初始化我们的向量存储。我们将使用以下属性对其进行初始化：\n",
    "        -   **嵌入维度 (Embedding Dimension)**: 每个文档块在转换为向量时将包含多少个数字。OpenAI的文本嵌入模型（我们将使用 `text-embedding-3-small`）会将任何文本转换为一个包含1536个数字的向量。\n",
    "        -   **索引 (Index)**: 就像书的目录一样，向量数据库中的索引是一个数据结构，它组织向量以便快速找到相似的向量。没有索引，查找相似向量将需要将你的查询与存储的每个向量逐一比较。有了索引，FAISS会预先组织向量，从而可以快速定位到最相似的那些。\n",
    "        -   **文档 (Documents)**: 在一个常规的Python列表中存储实际的文档块（原始文本加上元数据）。\n",
    "        -   **ID到索引的映射 (ID to Index)**: 这是一个字典，将文档块的ID映射到其在列表中的位置，以便我们可以通过ID快速找到它。\n",
    "    -   **添加文档函数 (`add_documents`)**: 接收一批文档块并将其存储到我们的向量存储中。我们处理每个已经向量化的文档，并将其添加到FAISS索引中以进行快速相似性搜索。\n",
    "    -   **搜索函数 (`search`)**: 接收一个用户的查询（已转换为1536维的向量），并找到最相似的文档块。\n",
    "3.  现在我们有了自定义的向量存储。接下来，我们需要将一个LLM连接到它，这就是Semantic Kernel发挥作用的地方。我们将使用Semantic Kernel来利用其**嵌入能力**（将文本转换为向量）和**聊天补全能力**（LLM的推理引擎）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "# 我们创建一个DocumentChunk类来表示文档的每个片段\n",
    "class DocumentChunk:\n",
    "    # 必需字段 - 每个分块都必须有这些\n",
    "    id: str                    # 此分块的唯一标识符，如 \"policy_doc_chunk_1\"\n",
    "    content: str               # 此分块的实际文本内容\n",
    "    source_doc_id: str         # 此分块来自哪个原始文档\n",
    "    title: str                 # 原始文档的可读标题\n",
    "    chunk_index: int           # 这是第几块？ (0=第一块, 1=第二块, 等等)\n",
    "   \n",
    "    # 可选字段 - 这些有默认值\n",
    "    department: str = \"\"       # 哪个团队拥有此文档 (可选)\n",
    "    doc_type: str = \"\"         # 这是什么类型的文档 - 政策、指南等 (可选)\n",
    "    embedding: List[float] = None  # 此文本的向量表示 (数字列表)\n",
    "\n",
    "# 这是一个包含搜索文档方法的类。它不是进行精确的词匹配，而是找到含义相似的文档。\n",
    "class SimpleVectorStore:\n",
    "   # 初始化我们的向量存储。我们将使用FAISS，它可以快速存储和搜索大量的文档分块。\n",
    "   def __init__(self, embedding_dimension: int = 1536):\n",
    "       # 每个嵌入向量中有多少个数字？这意味着每个文档块将由一个包含1536个数字的列表来表示其含义。\n",
    "       # OpenAI的 text-embedding-3-small 模型为每段文本提供1536个数字。\n",
    "       self.embedding_dimension = embedding_dimension\n",
    "       \n",
    "       # 创建一个FAISS索引来存储我们的文档嵌入。IndexFlatIP意味着我们将使用内积相似度（类似于余弦相似度）来查找相似的文档。\n",
    "       self.index = faiss.IndexFlatIP(embedding_dimension)\n",
    "       \n",
    "       # 在一个名为documents的列表中存储实际的文档块（文本和元数据）\n",
    "       self.documents: List[DocumentChunk] = []\n",
    "       \n",
    "       # 这个字典将文档ID映射到其在documents列表中的位置，以便我们能通过ID快速找到文档。\n",
    "       self.id_to_index = {}\n",
    "   \n",
    "   # 此函数将一批文档添加到我们的向量存储中。\n",
    "   def add_documents(self, documents: List[DocumentChunk]):\n",
    "       embeddings = []  # 这里我们将存储每个文档块的向量表示（嵌入）\n",
    "       \n",
    "       # 逐个处理每个文档\n",
    "       for doc in documents:\n",
    "           if doc.embedding is None:\n",
    "               raise ValueError(f\"文档 {doc.id} 缺少嵌入向量\")\n",
    "           \n",
    "           # 关键步骤：规范化嵌入向量\n",
    "           # 为什么？这样我们就可以使用余弦相似度（比较角度，而不是长度）\n",
    "           embedding_array = np.array(doc.embedding)  # 将列表转换为numpy数组以便进行数学运算\n",
    "           faiss.normalize_L2(embedding_array.reshape(1, -1)) # FAISS的L2规范化，使其长度为1\n",
    "           embeddings.append(embedding_array)\n",
    "           \n",
    "           doc_index = len(self.documents)           # 这个文档将处于什么位置？\n",
    "           self.documents.append(doc)                # 将文档添加到我们的存储中\n",
    "           self.id_to_index[doc.id] = doc_index      # 记住：这个ID在这个位置\n",
    "       \n",
    "       # 将所有规范化的向量添加到FAISS中以进行闪电般的快速搜索\n",
    "       embeddings_array = np.vstack(embeddings).astype('float32')  # FAISS需要float32类型\n",
    "       self.index.add(embeddings_array)\n",
    "       \n",
    "       print(f\"已向向量存储中添加 {len(documents)} 个文档。\")\n",
    "   \n",
    "   # 此函数搜索与用户问题相似的文档。\n",
    "   def search(self, query_embedding: List[float], k: int = 3, score_threshold: float = 0.0):\n",
    "       if self.index.ntotal == 0:\n",
    "           return []\n",
    "       \n",
    "       query_array = np.array(query_embedding).reshape(1, -1).astype('float32')\n",
    "       faiss.normalize_L2(query_array)\n",
    "       \n",
    "       scores, indices = self.index.search(query_array, k)\n",
    "       \n",
    "       results = []\n",
    "       for score, idx in zip(scores[0], indices[0]):\n",
    "           if idx != -1 and score >= score_threshold:\n",
    "               document = self.documents[idx]\n",
    "               results.append((document, float(score)))\n",
    "       \n",
    "       return results\n",
    "\n",
    "# 设置我们将使用的AI服务\n",
    "kernel = Kernel()  # Semantic Kernel是我们的AI编排框架\n",
    "\n",
    "# 服务1：聊天补全（生成对问题的回答）\n",
    "chat_service = OpenAIChatCompletion(\n",
    "   ai_model_id=\"gpt-4o\"\n",
    ")\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# 服务2：文本嵌入（将文本转换为向量表示）\n",
    "embedding_service = OpenAITextEmbedding(\n",
    "   ai_model_id=\"text-embedding-3-small\"\n",
    ")\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "print(\"Semantic Kernel已使用OpenAI服务进行初始化。\")\n",
    "print(\"使用FAISS的简单向量存储进行文档存储。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8700b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第一部分：演示问题——模型无法访问私有数据\n",
    "\n",
    "让我们首先展示当向一个AI模型询问它未曾训练过的信息时会发生什么。\n",
    "\n",
    "下面的代码很简单，我们在一系列数组中存储了“文档”。我们将对这些文档提出问题（但我们实际上不会实现一个完整的RAG系统），所以我们预期模型会不知道我们在说什么。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c18a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型不知道的示例公司数据。\n",
    "# 这代表了私有的、“地面真实”的信息。\n",
    "company_documents = [\n",
    "    {\n",
    "        \"id\": \"product_001\",\n",
    "        \"title\": \"CloudSync Pro企业版计划\",\n",
    "        \"content\": \"\"\"CloudSync Pro企业版提供无限存储、高级加密、最多500用户的实时协作、优先支持和自定义集成。定价：每用户每月49美元，需年度承诺。功能包括：自动备份、版本控制、审计日志、SSO集成和99.9%的正常运行时间SLA。\"\"\",\n",
    "        \"metadata\": {\"department\": \"产品部\", \"type\": \"定价\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"policy_001\", \n",
    "        \"title\": \"2024年远程工作政策\",\n",
    "        \"content\": \"\"\"自2024年1月起生效：所有员工每周最多可远程工作3天。远程工作需经直接经理批准。每年提供500美元的设备津贴用于家庭办公室设置。团队会议必须使用视频通话。核心协作时间：当地时间上午10点至下午3点。\"\"\",\n",
    "        \"metadata\": {\"department\": \"人力资源部\", \"type\": \"政策\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"process_001\",\n",
    "        \"title\": \"客户退款流程\",\n",
    "        \"content\": \"\"\"步骤1：客户通过支持门户提交退款请求。步骤2：支持代理在24小时内审核。步骤3：金额低于100美元的，自动批准。步骤4：金额超过100美元的，需要经理批准。步骤5：退款将在3-5个工作日内处理至原支付方式。购买后30天内可全额退款。\"\"\",\n",
    "        \"metadata\": {\"department\": \"支持部\", \"type\": \"流程\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"guide_001\",\n",
    "        \"title\": \"新员工入职清单\",\n",
    "        \"content\": \"\"\"第一天：IT设置和系统访问。第二天：部门介绍和导师分配。第一周：完成强制性培训模块（安全、合规、公司文化）。第二周：跟随团队成员并审查项目文档。第一个月：完成试用期审查并设定90天目标。\"\"\",\n",
    "        \"metadata\": {\"department\": \"人力资源部\", \"type\": \"指南\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# 我们想用来测试模型基础知识的问题。\n",
    "test_questions = [\n",
    "    \"CloudSync Pro企业版的价格是多少？\",\n",
    "    \"员工每周可以远程工作几天？\",\n",
    "    \"购买金额超过100美元的退款审批流程是怎样的？\",\n",
    "    \"员工入职第一周会发生什么？\"\n",
    "]\n",
    "\n",
    "async def run_direct_to_model_test():\n",
    "    \"\"\"\n",
    "    直接向基础AI模型测试问题，以证明其对我们私有公司数据缺乏了解。\n",
    "    \"\"\"\n",
    "    print(\"测试无RAG的模型 - 关于私有公司数据的问题:\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    chat_service = kernel.get_service(type=OpenAIChatCompletion)\n",
    "\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(model_id=\"gpt-4o\")\n",
    "\n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n问题 {i}: {question}\")\n",
    "        \n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(question)\n",
    "\n",
    "        response = await chat_service.get_chat_message_content(\n",
    "            chat_history=chat_history,\n",
    "            settings=execution_settings\n",
    "        )\n",
    "        \n",
    "        print(f\"模型响应: {str(response)}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(\"✅ 开始测试...\")\n",
    "await run_direct_to_model_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50323873",
   "metadata": {},
   "source": [
    "## 我们刚才观察到了什么\n",
    "\n",
    "模型要么：\n",
    "1. **无法回答**，因为它无法访问这家特定公司的信息。\n",
    "2. **提供通用性回答**，可能与你的实际政策不符。\n",
    "3. **做出假设**，这些假设在你的特定情境下可能是错误的。\n",
    "\n",
    "这正是我们需要RAG的原因——在保留其推理能力的同时，赋予模型访问你特定数据的能力。\n",
    "\n",
    "---\n",
    "\n",
    "# 第二部分：文档分块策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在本节中，我们将探讨不同的文本分块策略。\n",
    "# 我们可以进行简单的基于字符的分割，或者更智能的、尊重段落和句子的语义分割。\n",
    "\n",
    "def simple_text_splitter(text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"简单的基于字符的文本分割器，带重叠功能\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "        if start >= len(text): break\n",
    "    return chunks\n",
    "\n",
    "def semantic_text_splitter(text: str, max_chunk_size: int = 400) -> List[str]:\n",
    "    \"\"\"尊重段落和句子边界的文本分割器\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\"\n",
    "    for paragraph in paragraphs:\n",
    "        if len(current_chunk) + len(paragraph) + 2 > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            current_chunk += (\"\\n\" + paragraph) if current_chunk else paragraph\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "# 测试不同的分块策略\n",
    "sample_doc = company_documents[0]\n",
    "print(\"分块策略比较:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(f\"原始文档: {sample_doc['title']}\")\n",
    "print(f\"长度: {len(sample_doc['content'])} 字符\")\n",
    "\n",
    "print(\"\\n1. 简单的基于字符的分块:\")\n",
    "simple_chunks = simple_text_splitter(sample_doc['content'], chunk_size=200, overlap=30)\n",
    "for i, chunk in enumerate(simple_chunks):\n",
    "    print(f\"块 {i+1} ({len(chunk)} 字符): {chunk}\")\n",
    "\n",
    "print(\"\\n2. 语义分块 (尊重段落):\")\n",
    "semantic_chunks = semantic_text_splitter(sample_doc['content'], max_chunk_size=250)\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"块 {i+1} ({len(chunk)} 字符): {chunk}\")\n",
    "\n",
    "print(\"\\n权衡利弊:\")\n",
    "print(\"- 简单分块: 大小可预测，但可能在句子中间断开。\")\n",
    "print(\"- 语义分块: 保持意义完整，但分块大小可变。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b2f8b",
   "metadata": {},
   "source": [
    "## 测试完整的RAG流程\n",
    "\n",
    "现在，我们将实现一个简单的RAG系统，它使用Semantic Kernel来处理文档检索和答案生成。\n",
    "\n",
    "1.  **semantic_chunker**: 它的工作是将一段文本分割成更小的字符串（块）。它首先根据段落（由两个换行符 `\n\n` 分隔）来分割文本，这确保了属于一起的句子能保持在一起。然后，它遍历这些段落，根据最大块大小将它们组合成块。通过尊重文本的自然断点，它确保了相关句子保持在一起，从而创建了高质量、重点突出的信息块。这极大地提高了我们数据的“信噪比”。\n",
    "2.  **ingest_documents_semantic**: 这个函数旨在解决任何RAG系统的第一个主要问题：为AI准备数据。它接收一个文档列表、一个向量存储（我们之前构建的自定义数据库）和嵌入服务。它将这些文档转换为向量，遍历每个文档，调用嵌入服务，并创建一个`DocumentChunk`对象（包含向量、原始文本和元数据）。\n",
    "3.  **ask_with_semantic_rag**: RAG的核心引擎。它接收用户的问题、内核和向量存储（我们的知识库）。它通过嵌入服务传递用户问题以获得向量表示，然后使用向量搜索方法找到向量最接近的文档块。然后我们增强提示并最终生成答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 语义分块的辅助函数 ---\n",
    "def semantic_chunker(text: str, max_chunk_size: int = 300) -> List[str]:\n",
    "    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\"\n",
    "    for paragraph in paragraphs:\n",
    "        if current_chunk and (len(current_chunk) + len(paragraph) + 2) > max_chunk_size:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            current_chunk += (\"\\n\\n\" + paragraph) if current_chunk else paragraph\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "# --- 核心RAG函数 ---\n",
    "async def ingest_documents_semantic(documents: List[Dict], vector_store: SimpleVectorStore, embedding_service: OpenAITextEmbedding) -> None:\n",
    "    print(f\"使用语义分块处理 {len(documents)} 个文档...\")\n",
    "    all_chunks_to_add = []\n",
    "    for doc in documents:\n",
    "        text_chunks = semantic_chunker(doc[\"content\"])\n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            if len(chunk_text) < 20: continue\n",
    "            embedding = (await embedding_service.generate_embedding(chunk_text))\n",
    "            chunk = DocumentChunk(id=f\"{doc['id']}_chunk_{i}\", content=chunk_text, source_doc_id=doc[\"id\"], title=doc[\"title\"], chunk_index=i, embedding=embedding)\n",
    "            all_chunks_to_add.append(chunk)\n",
    "    vector_store.add_documents(all_chunks_to_add)\n",
    "    print(f\"已向向量存储中添加 {len(all_chunks_to_add)} 个新块。\")\n",
    "\n",
    "async def ask_with_semantic_rag(question: str, kernel: Kernel, vector_store: SimpleVectorStore) -> str:\n",
    "    embedding_service = kernel.get_service(type=OpenAITextEmbedding)\n",
    "    chat_service = kernel.get_service(type=OpenAIChatCompletion)\n",
    "    \n",
    "    query_embedding = (await embedding_service.generate_embedding(question))\n",
    "    search_results = vector_store.search(query_embedding, k=3, score_threshold=0.3)\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"我在文档中找不到任何相关信息来回答这个问题。\"\n",
    "        \n",
    "    context = \"\\n\\n---\\n\\n\".join([result.content for result, score in search_results])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "仅根据下面提供的上下文回答以下问题。\n",
    "\n",
    "上下文:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "问题: {question}\n",
    "\n",
    "答案:\n",
    "\"\"\"\n",
    "\n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_user_message(prompt)\n",
    "    \n",
    "    settings = OpenAIChatPromptExecutionSettings(model_id=\"gpt-4o\", max_tokens=200, temperature=0.1)\n",
    "    \n",
    "    response = await chat_service.get_chat_message_content(chat_history, settings)\n",
    "    \n",
    "    return str(response)\n",
    "\n",
    "# --- 主执行块 ---\n",
    "semantic_vector_store = SimpleVectorStore()\n",
    "embedding_service = kernel.get_service(type=OpenAITextEmbedding)\n",
    "await ingest_documents_semantic(company_documents, semantic_vector_store, embedding_service)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"测试使用语义分块的RAG系统:\")\n",
    "question_to_ask = \"CloudSync Pro企业版的价格是多少？\"\n",
    "answer = await ask_with_semantic_rag(question_to_ask, kernel, semantic_vector_store)\n",
    "\n",
    "print(f\"\\n问: {question_to_ask}\")\n",
    "print(f\"答: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cad6f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第四部分：高级配置与调优\n",
    "\n",
    "我们正在测试两种简单的方法来让我们的RAG系统给出更好的答案——第一种是通过改变我们要求AI回应的方式（友好型 vs 专业型），第二种是通过调整我们对包含哪些文档的挑剔程度（严格匹配 vs 宽松匹配）。\n",
    "\n",
    "后者被称为**相似度阈值**。相似度阈值就像是为“相关”搜索结果设定的门槛。它是一个介于0和1之间的数字，决定了一个文档块必须与你的问题有多相似，我们才会将其包含在答案中。\n",
    "\n",
    "当阈值**过低**时：如果你用0.2的阈值问“我们的休假政策是什么？”，你可能会得到关于休假政策、员工福利、工时追踪和公司假期的结果。虽然都与人力资源相关，但这会给用户带来大量并非直接回答他们问题的信息。然后，AI不得不在所有这些额外的上下文中筛选，可能会稀释最终答案的质量。\n",
    "\n",
    "当阈值**过高**时：如果你用0.7的阈值问“我如何申请休假？”，你可能根本得不到任何结果，因为没有文档包含那个确切的短语，即使你的休假政策文档清楚地解释了流程。当答案实际上存在于你的知识库中时，用户最终会因“未找到信息”的响应而感到沮丧。\n",
    "\n",
    "**找到最佳点**：目标是找到一个既能提供足够相关信息又不会引入噪音的阈值。对于大多数商业文档，0.3到0.5之间的阈值效果很好——足够高以过滤掉不相关的内容，但又足够低以捕捉到可能使用不同措辞的相关信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82976191",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 最佳实践总结\n",
    "\n",
    "### 文档处理\n",
    "- **使用语义分块**，尊重段落和句子边界。\n",
    "- **最佳块大小**: 对于大多数商业文档，300-400个字符。\n",
    "- **包含有意义的重叠** (50-80个字符)以保持上下文。\n",
    "- **保留丰富的元数据**用于过滤和来源归属。\n",
    "\n",
    "### 向量搜索配置\n",
    "- **从FAISS开始**进行本地开发和小型生产。\n",
    "- **使用0.3左右的相似度阈值**以平衡精度/召回率。\n",
    "- **检索3-5个文档**以提供足够的上下文而无噪音。\n",
    "- **规范化嵌入**以进行一致的相似度计算。\n",
    "\n",
    "### 提示工程\n",
    "- **为不同用户类型创建特定角色的提示** (客户、员工、高管)。\n",
    "- **包含清晰的指令**以处理信息不可用的情况。\n",
    "- **使用结构化模板**将上下文与问题分开。\n",
    "- **测试提示变体**以优化你的特定用例。\n",
    "\n",
    "## 下一步\n",
    "\n",
    "1. **从核心功能开始** - 让基本的RAG与你的文档一起工作。\n",
    "2. **尽早添加监控** - 实现日志记录和指标收集。\n",
    "3. **为你的领域定制** - 为你的内容量身定制提示和分块。\n",
    "4. **根据反馈迭代** - 使用真实的用户互动来改进系统。\n",
    "5. **为生产做计划** - 考虑可扩展性、监控和维护。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}